#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced SutonnyMJ to Unicode Converter
Based on actual SutonnyMJ font mapping analysis
"""

import re

def convert_sutonnymj_to_unicode(text):
    """
    Convert SutonnyMJ encoded text to proper Unicode Bangla
    """
    
    # Skip if line is English
    if is_english_text(text):
        return text
    
    # Apply SutonnyMJ to Unicode conversion
    result = text
    
    # Step 1: Handle compound characters first
    compound_mappings = {
        # Common SutonnyMJ patterns observed in the text
        'kvw┼б├Н': 'рж╢рж╛ржирзНрждрж┐',
        'mvsevw`KZv': 'рж╕рж╛ржВржмрж╛ржжрж┐ржХрждрж╛',
        'PP┬йv': 'ржЪрж░рзНржЪрж╛',
        'cwiтАбc├Цw┬╢Z': 'ржкрж░рж┐ржкрзНрж░рзЗржХрзНрж╖рж┐ржд',
        'Kvk┬еxi': 'ржХрж╛рж╢ржорзАрж░',
        'Bmy┬и': 'ржЗрж╕рзНржпрзБ',
        'тАаivevтАбqZ': 'рж░рзЛржмрж╛ржпрж╝рзЗржд',
        'тАаdiтАб`┼аm': 'ржлрзЗрж░ржжрзМрж╕',
        'Zb┬еq': 'рждржирзНржоржпрж╝',
        'mvnv': 'рж╕рж╛рж╣рж╛',
        'Rq': 'ржЬржпрж╝',
        'gyL┬иk├г': 'ржорзБржЦрзНржпрж╢ржмрзНржж',
        'MYgva┬иg': 'ржЧржгржорж╛ржзрзНржпржо',
        'hy├ЧgyLxbZv': 'ржпрзБржжрзНржзржорзБржЦрж┐ржирждрж╛',
        'Aa┬иvcK': 'ржЕржзрзНржпрж╛ржкржХ',
        'MYтАбhvMvтАбhvM': 'ржЧржгржпрзЛржЧрж╛ржпрзЛржЧ',
        'wefvM': 'ржмрж┐ржнрж╛ржЧ',
        'XvKv': 'ржврж╛ржХрж╛',
        'wek^we`┬иvjq': 'ржмрж┐рж╢рзНржмржмрж┐ржжрзНржпрж╛рж▓ржпрж╝',
        'c├ЦfvlK': 'ржкрзНрж░ржнрж╛рж╖ржХ',
        'KwgDwbтАбKkb': 'ржХржорж┐ржЙржирж┐ржХрзЗрж╢ржи',
        'A┬иv├Ы': 'ржЕрзНржпрж╛ржирзНржб',
        'gvw├лwgwWqv': 'ржорж╛рж▓рзНржЯрж┐ржорж┐ржбрж┐ржпрж╝рж╛',
        'Rvb┬йvwjRg': 'ржЬрж╛рж░рзНржирж╛рж▓рж┐ржЬржо',
        'Bmjvgx': 'ржЗрж╕рж▓рж╛ржорзА',
        'Kzw├│qv': 'ржХрзБрж╖рзНржЯрж┐ржпрж╝рж╛',
        'cw├ОKv': 'ржкрждрзНрж░рж┐ржХрж╛',
        'msL┬иv': 'рж╕ржВржЦрзНржпрж╛',
        'hy┬│': 'ржпрзБржХрзНржд',
        'тАаmтАб├Ю┬д^i': 'рж╕рзЗржкрзНржЯрзЗржорзНржмрж░',
        'f~wgKv': 'ржнрзВржорж┐ржХрж╛',
        'fviZxq': 'ржнрж╛рж░рждрзАржпрж╝',
        'DcgnvтАб`k': 'ржЙржкржорж╣рж╛ржжрзЗрж╢',
        '┬╕i├жZ┬бc~Y┬й': 'ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг',
        'A├Вj': 'ржЕржЮрзНржЪрж▓',
        'fviZ': 'ржнрж╛рж░ржд',
        'cvwK┬п├Нvb': 'ржкрж╛ржХрж┐рж╕рзНрждрж╛ржи',
        'Pxb': 'ржЪрзАржи',
        'wZb': 'рждрж┐ржи',
        'тАа`k': 'ржжрзЗрж╢',
        'gvтАбS': 'ржорж╛ржЭрзЗ',
        'we┬п├НтАжZ': 'ржмрж┐рж╕рзНрждрзГржд',
        'wefvR': 'ржмрж┐ржнрж╛ржЧ',
        'ci': 'ржкрж░',
        'тАа_тАбK': 'ржерзЗржХрзЗ',
        'GB': 'ржПржЗ',
        'wewfb┼У': 'ржмрж┐ржнрж┐ржирзНржи',
        'ivRтА░bwZK': 'рж░рж╛ржЬржирзИрждрж┐ржХ',
        'ag┬йxq': 'ржзрж░рзНржорзАржпрж╝',
        'BZ┬иvw`': 'ржЗрждрзНржпрж╛ржжрж┐',
        'Bmy┬итАбZ': 'ржЗрж╕рзНржпрзБрждрзЗ',
        'AvтАбjvPbvq': 'ржЖрж▓рзЛржЪржирж╛ржпрж╝',
        'GтАбmтАбQ': 'ржПрж╕рзЗржЫрзЗ',
        'me┬йтАбkl': 'рж╕рж░рзНржмрж╢рзЗрж╖',
        'wbqw┼б┬┐Z': 'ржирж┐ржпрж╝ржирзНрждрзНрж░рж┐ржд',
        'Kvk┬еxтАбi': 'ржХрж╛рж╢ржорзАрж░рзЗ',
        'weтАбkl': 'ржмрж┐рж╢рзЗрж╖',
        'gh┬йv`v': 'ржорж░рзНржпрж╛ржжрж╛',
        'wejy├Я': 'ржмрж┐рж▓рзБржкрзНржд',
        'KтАбi': 'ржХрж░рзЗ',
        'welтАбq': 'ржмрж┐рж╖ржпрж╝рзЗ',
        'evsjvтАб`k': 'ржмрж╛ржВрж▓рж╛ржжрзЗрж╢',
        'mivmwi': 'рж╕рж░рж╛рж╕рж░рж┐',
        'hy┬│': 'ржпрзБржХрзНржд',
        'bv': 'ржирж╛',
        '_vKтАбjI': 'ржерж╛ржХрж▓рзЗржУ',
        'f~': 'ржнрзВ',
        'ivRтА░bwZK': 'рж░рж╛ржЬржирзИрждрж┐ржХ',
        'KviтАбY': 'ржХрж╛рж░ржгрзЗ',
        'Bmy┬иwU': 'ржЗрж╕рзНржпрзБржЯрж┐',
        'AZ┬и┼б├Н': 'ржЕрждрзНржпржирзНржд',
        '┬╕i├жZ┬бc~Y┬й': 'ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг',
        'cvkvcvwk': 'ржкрж╛рж╢рж╛ржкрж╛рж╢рж┐',
        'emevmKvix': 'ржмрж╛рж╕ржмрж╛рж╕ржХрж╛рж░рзА',
        'RbтАбMv├┤x': 'ржЬржиржЧрзЛрж╖рзНржарзА',
        'AwaKvsk': 'ржЕржзрж┐ржХрж╛ржВрж╢',
        'gvbyl': 'ржорж╛ржирзБрж╖',
        'gymwjg': 'ржорзБрж╕рж▓рж┐ржо',
        'nIqvq': 'рж╣ржУржпрж╝рж╛ржпрж╝',
        'GтАб`k': 'ржПржжрзЗрж╢',
        'gvbyтАбli': 'ржорж╛ржирзБрж╖рзЗрж░',
        'mтАб┬╜': 'рж╕рж╛ржерзЗ',
        'GK': 'ржПржХ',
        'aiтАбbi': 'ржзрж░ржирзЗрж░',
        'gb┬п├Нvw├Л┬бK': 'ржоржирж╕рзНрждрж╛рждрзНрждрзНржмрж┐ржХ',
        '╦ЖbKтАбU┬иi': 'ржирзИржХржЯрзНржпрзЗрж░',
        'welqwU': 'ржмрж┐рж╖ржпрж╝ржЯрж┐',
        'weтАбeP┬и': 'ржмрж┐ржмрзЗржЪрзНржп',
        'wejy├Я': 'ржмрж┐рж▓рзБржкрзНржд',
        'Kivi': 'ржХрж░рж╛рж░',
        'mтАЮ├│': 'рж╕рзГрж╖рзНржЯ',
        'eZ┬йgvb': 'ржмрж░рзНрждржорж╛ржи',
        'Bmy┬иwU': 'ржЗрж╕рзНржпрзБржЯрж┐',
        'ZvB': 'рждрж╛ржЗ',
        'evsjvтАб`wk': 'ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рж┐',
        'cvVKтАб`i': 'ржкрж╛ржаржХржжрзЗрж░',
        'KvтАбQ': 'ржХрж╛ржЫрзЗ',
        'AтАбbK': 'ржЕржирзЗржХ',
        'тАа`тАбki': 'ржжрзЗрж╢рзЗрж░',
        'c├Цvq': 'ржкрзНрж░рж╛ржпрж╝',
        'me': 'рж╕ржм',
        'msev`c├О': 'рж╕ржВржмрж╛ржжржкрждрзНрж░',
        'G': 'ржП',
        'welтАбq': 'ржмрж┐рж╖ржпрж╝рзЗ',
        'wbqwgZ': 'ржирж┐ржпрж╝ржорж┐ржд',
        'c├ЦwZтАбe`b': 'ржкрзНрж░рждрж┐ржмрзЗржжржи',
        'c├ЦKvk': 'ржкрзНрж░ржХрж╛рж╢',
        'KтАбiтАбQ': 'ржХрж░рзЗржЫрзЗ',
        'AbjvBb': 'ржЕржирж▓рж╛ржЗржи',
        'MYgva┬иgMyтАбjvтАбZ': 'ржЧржгржорж╛ржзрзНржпржоржЧрзБрж▓рзЛрждрзЗ',
        'I': 'ржУ',
        'wbqwgZ': 'ржирж┐ржпрж╝ржорж┐ржд',
        'G': 'ржП',
        'Bmy┬итАбZ': 'ржЗрж╕рзНржпрзБрждрзЗ',
        'msev`': 'рж╕ржВржмрж╛ржж',
        'c├ЦKvwkZ': 'ржкрзНрж░ржХрж╛рж╢рж┐ржд',
        'nтАбqтАбQ': 'рж╣ржпрж╝рзЗржЫрзЗ',
        'cvVKmsL┬иvi': 'ржкрж╛ржаржХрж╕ржВржЦрзНржпрж╛рж░',
        'weтАбePbvq': 'ржмрж┐ржмрзЗржЪржирж╛ржпрж╝',
        'wewWwbDR24': 'ржмрж┐ржбрж┐ржирж┐ржЙржЬ24',
        'Kg': 'ржХржо',
        'I': 'ржУ',
        'c├Ц_g': 'ржкрзНрж░ржержо',
        'AvтАбjv': 'ржЖрж▓рзЛ',
        'AbjvBb': 'ржЕржирж▓рж╛ржЗржи',
        'тАа`тАбki': 'ржжрзЗрж╢рзЗрж░',
        'AbjvBb': 'ржЕржирж▓рж╛ржЗржи',
        'MYgva┬иgMyтАбjvi': 'ржЧржгржорж╛ржзрзНржпржоржЧрзБрж▓рзЛрж░',
        'gтАбa┬и': 'ржоржзрзНржпрзЗ',
        'Ab┬иZg': 'ржЕржирзНржпрждржо',
        '┬╕i├жZ┬бc~Y┬й': 'ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг',
        'eZ┬йgvb': 'ржмрж░рзНрждржорж╛ржи',
        'MтАбelYvwU': 'ржЧржмрзЗрж╖ржгрж╛ржЯрж┐',
        'cwiPvjbvi': 'ржкрж░рж┐ржЪрж╛рж▓ржирж╛рж░',
        'gva┬итАбg': 'ржорж╛ржзрзНржпржорзЗ',
        'kvw┼б├Н': 'рж╢рж╛ржирзНрждрж┐',
        'mvsevw`KZv': 'рж╕рж╛ржВржмрж╛ржжрж┐ржХрждрж╛',
        'gтАбWтАбji': 'ржоржбрзЗрж▓рзЗрж░',
        'AvтАбjvтАбK': 'ржЖрж▓рзЛржХрзЗ',
        'wewWwbDR24': 'ржмрж┐ржбрж┐ржирж┐ржЙржЬ24',
        'Kg': 'ржХржо',
        'I': 'ржУ',
        'c├Ц_g': 'ржкрзНрж░ржержо',
        'AvтАбjv': 'ржЖрж▓рзЛ',
        'AbjvBтАбb': 'ржЕржирж▓рж╛ржЗржирзЗ',
        'Kvk┬еxi': 'ржХрж╛рж╢ржорзАрж░',
        'Bmy┬итАбZ': 'ржЗрж╕рзНржпрзБрждрзЗ',
        'c├ЦKvwkZ': 'ржкрзНрж░ржХрж╛рж╢рж┐ржд',
        'msev`MyтАбjvi': 'рж╕ржВржмрж╛ржжржЧрзБрж▓рзЛрж░',
        'cwiтАбekbv': 'ржкрж░рж┐ржмрзЗрж╢ржирж╛',
        'I': 'ржУ',
        'c├ЦKтАжwZ': 'ржкрзНрж░ржХрзГрждрж┐',
        'тАа`Lv': 'ржжрзЗржЦрж╛',
        'nтАбqтАбQ': 'рж╣ржпрж╝рзЗржЫрзЗ'
    }
    
    # Apply compound mappings first
    for sutonnymj, unicode_bangla in compound_mappings.items():
        result = result.replace(sutonnymj, unicode_bangla)
    
    # Step 2: Handle remaining individual characters
    char_mappings = {
        # Basic consonants
        'K': 'ржХ', 'L': 'ржЦ', 'M': 'ржЧ', 'N': 'ржШ', 'O': 'ржЩ',
        'P': 'ржЪ', 'Q': 'ржЫ', 'R': 'ржЬ', 'S': 'ржЭ', 'T': 'ржЮ',
        'U': 'ржЯ', 'V': 'ржа', 'W': 'ржб', 'X': 'ржв', 'Y': 'ржг',
        'Z': 'ржд', '_': 'рже', '`': 'ржж', 'a': 'ржз', 'b': 'ржи',
        'c': 'ржк', 'd': 'ржл', 'e': 'ржм', 'f': 'ржн', 'g': 'ржо',
        'h': 'ржп', 'i': 'рж░', 'j': 'рж▓', 'k': 'рж╢', 'l': 'рж╖',
        'm': 'рж╕', 'n': 'рж╣', 'o': 'ржбрж╝', 'p': 'ржврж╝', 'q': 'ржпрж╝',
        'r': 'рзО', 's': 'ржВ', 't': 'ржГ', 'u': 'ржБ',
        
        # Vowel marks
        'v': 'рж╛', 'w': 'рж┐', 'x': 'рзА', 'y': 'рзБ', 'z': 'рзВ',
        'тАа': 'рзЗ', 'тАб': 'рзЛ', '╦Ж': 'рзИ', '┬й': 'рж░рзН', '┬и': 'рзЧ',
        '┬п': 'рзН', '┬░': 'ржВ', '┬▒': 'ржГ', '┬▓': 'ржБ',
        
        # Special characters
        '┬е': 'ржп', '├Н': 'рзА', '┼б': 'ржи', '├Ц': 'ржкрзНрж░', '┬╢': 'ржк',
        
        # Numbers
        '0': 'рзж', '1': 'рзз', '2': 'рзи', '3': 'рзй', '4': 'рзк',
        '5': 'рзл', '6': 'рзм', '7': 'рзн', '8': 'рзо', '9': 'рзп'
    }
    
    # Apply character mappings
    for sutonnymj_char, unicode_char in char_mappings.items():
        result = result.replace(sutonnymj_char, unicode_char)
    
    return result

def is_english_text(text):
    """Check if text is primarily English"""
    text = text.strip()
    if not text:
        return True
    
    # English indicators
    english_words = [
        'abstract', 'in this study', 'the nature', 'representation',
        'news published', 'newspapers', 'analysed using', 'ideas',
        'peace journalism', 'model given', 'norwegian', 'social scientist',
        'johan galtung', 'coding frames', 'prepared', 'light',
        'framing analysis', 'content analysis', 'method', 'used',
        'considering', 'overall results', 'research', 'found',
        'bangladeshi media', 'still dependent', 'traditional style',
        'journalism', 'reporting war', 'conflicts', 'practice',
        'large scale', 'evidence', 'conventional trends',
        'sustaining', 'contributing', 'shaping', 'problem',
        'greater degree', 'however', 'proven', 'vital role',
        'resolving', 'establishing peace', 'issn', 'doi', 'http',
        'ministry of law', 'buchanan', 'mustafa', 'ottosen',
        'lee', 'maslog', 'khan', 'sheuli', 'galtung', 'lynch',
        'mcgoldrick', 'pan', 'kosicki', 'goffman', 'creswell',
        'facebook', 'alexa', 'transcend', 'harper', 'row',
        'sage publications', 'universidad', 'new york',
        'gloucestershire', 'hawthorn press', 'doi', 'retrieved'
    ]
    
    text_lower = text.lower()
    
    # Check for English keywords
    for word in english_words:
        if word in text_lower:
            return True
    
    # Check ASCII ratio
    ascii_count = sum(1 for c in text if ord(c) < 128)
    total_chars = len(text)
    
    if total_chars > 0 and ascii_count / total_chars > 0.8:
        return True
    
    return False

def convert_file_smart(input_file, output_file):
    """Convert file intelligently preserving English content"""
    print(f"ЁЯОп Smart conversion: {input_file} тЖТ {output_file}")
    
    with open(input_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    converted_lines = []
    for i, line in enumerate(lines):
        original = line.rstrip('\n')
        
        if is_english_text(original):
            converted_lines.append(original)
            if i < 15:
                print(f"Line {i+1} [EN]: {original[:60]}...")
        else:
            converted = convert_sutonnymj_to_unicode(original)
            converted_lines.append(converted)
            if i < 15:
                print(f"Line {i+1} [BN]: {original[:30]} тЖТ {converted[:40]}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(converted_lines))
    
    print(f"тЬЕ Smart conversion completed! {len(converted_lines)} lines processed")

if __name__ == "__main__":
    convert_file_smart('extracted_text_002.txt', 'extracted_text_002_final.txt')